"""
RAG í†µí•© ëª¨ë“ˆ (Vector Search Manager)

í¬ë¡œë§ˆ DBì™€ ì‹¤ì‹œê°„ PDF ê²€ìƒ‰ì„ í†µí•©í•˜ì—¬ ë³‘ë ¬ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³ 
ê²°ê³¼ë¥¼ í†µí•©, ë­í‚¹, ì¤‘ë³µ ì œê±°í•˜ëŠ” í•µì‹¬ ëª¨ë“ˆì…ë‹ˆë‹¤.
"""

from __future__ import annotations

import time
import asyncio
from typing import List, Dict, Any, Optional, Tuple, Union
from dataclasses import dataclass, field
import numpy as np

from utils.logger import log
from utils.embeddings import KoreanEmbeddingModel, create_embedding_model
from config.settings import settings
from .chroma_client import ChromaDBClient, SearchResult
from .pdf_processor import PDFProcessor, PDFChunk, create_pdf_processor


@dataclass
class UnifiedSearchResult:
    """RAG ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„° êµ¬ì¡°"""

    content: str
    source_type: str  # 'chroma', 'pdf_realtime' (RAG ì „ìš©)
    source_id: str
    metadata: Dict[str, Any]
    similarity_score: float
    rank: int
    context_length: int


@dataclass
class SearchContext:
    """ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ ì •ë³´"""

    query: str
    query_embedding: List[float]
    max_results: int
    similarity_threshold: float
    max_context_length: int
    search_start_time: float = field(default_factory=time.time)


class VectorSearchManager:
    """RAG ë²¡í„° ê²€ìƒ‰ ê´€ë¦¬ì

    í¬ë¡œë§ˆ DBì™€ ì‹¤ì‹œê°„ PDF ê²€ìƒ‰ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ê³ 
    RAG ê²°ê³¼ë¥¼ í†µí•©í•˜ì—¬ ìµœì ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

    ì£¼ì˜: ì´ í´ë˜ìŠ¤ëŠ” RAG ê²€ìƒ‰ ì „ìš©ì´ë©°, MCPë‚˜ Google SearchëŠ” í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    """

    def __init__(
        self,
        chroma_client: Optional[ChromaDBClient] = None,
        pdf_processor: Optional[PDFProcessor] = None,
        embedding_model: Optional[KoreanEmbeddingModel] = None,
        max_workers: Optional[int] = None,
        similarity_threshold: Optional[float] = None,
        max_context_length: Optional[int] = None,
    ) -> None:
        """ë²¡í„° ê²€ìƒ‰ ê´€ë¦¬ìë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        Args:
            chroma_client: í¬ë¡œë§ˆ DB í´ë¼ì´ì–¸íŠ¸
            pdf_processor: PDF í”„ë¡œì„¸ì„œ
            embedding_model: ì„ë² ë”© ëª¨ë¸ (ê³µìœ  ì¸ìŠ¤í„´ìŠ¤ ê¶Œì¥)
            max_workers: ë³‘ë ¬ ì²˜ë¦¬ ìµœëŒ€ ì›Œì»¤ ìˆ˜
            similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
            max_context_length: ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
        """
        try:
            # ì„¤ì •ê°’ ì ìš©
            self.similarity_threshold = (
                similarity_threshold or settings.rag.SIMILARITY_THRESHOLD
            )
            self.max_context_length = (
                max_context_length or settings.rag.MAX_CONTEXT_LENGTH
            )
            self.max_workers = max_workers or 4

            # í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” (ì„ë² ë”© ëª¨ë¸ì€ ì™¸ë¶€ì—ì„œ ì „ë‹¬ë°›ì€ ê²ƒ ìš°ì„  ì‚¬ìš©)
            if embedding_model is not None:
                self.embedding_model = embedding_model
                log.info("ì™¸ë¶€ì—ì„œ ì „ë‹¬ë°›ì€ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©")
            else:
                self.embedding_model = create_embedding_model()
                log.info("ìƒˆë¡œìš´ ì„ë² ë”© ëª¨ë¸ ìƒì„±")

            self.chroma_client = chroma_client or ChromaDBClient()

            # PDFProcessorë„ ë™ì¼í•œ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •
            if pdf_processor is not None:
                self.pdf_processor = pdf_processor
                # PDFProcessorì˜ ì„ë² ë”© ëª¨ë¸ì´ í˜„ì¬ ëª¨ë¸ê³¼ ë‹¤ë¥¸ ê²½ìš° ê²½ê³ 
                if (
                    hasattr(self.pdf_processor, "embedding_model")
                    and self.pdf_processor.embedding_model is not self.embedding_model
                ):
                    log.warning("PDFProcessorì™€ ë‹¤ë¥¸ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš© ì¤‘")
            else:
                self.pdf_processor = create_pdf_processor(
                    embedding_model=self.embedding_model
                )
                log.info("PDFProcessorì— ë™ì¼í•œ ì„ë² ë”© ëª¨ë¸ ì „ë‹¬")

            log.info(
                f"VectorSearchManager ì´ˆê¸°í™” ì™„ë£Œ: "
                f"threshold={self.similarity_threshold}, "
                f"max_context={self.max_context_length}, "
                f"workers={self.max_workers}, "
                f"embedding_model_id={id(self.embedding_model)}"
            )
        except Exception as e:
            log.error(f"VectorSearchManager ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    async def search_async(
        self,
        query: str,
        max_results: Optional[int] = None,
        similarity_threshold: Optional[float] = None,
        include_pdf_realtime: bool = True,
        include_chroma: bool = True,
    ) -> List[UnifiedSearchResult]:
        """ë¹„ë™ê¸° RAG ë²¡í„° ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜
            similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
            include_pdf_realtime: ì‹¤ì‹œê°„ PDF ê²€ìƒ‰ í¬í•¨ ì—¬ë¶€
            include_chroma: í¬ë¡œë§ˆ DB ê²€ìƒ‰ í¬í•¨ ì—¬ë¶€

        Returns:
            RAG ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (í¬ë¡œë§ˆ DB + ì‹¤ì‹œê°„ PDF)
        """
        try:
            search_start = time.time()
            log.info(f"ğŸ” ë¹„ë™ê¸° RAG ê²€ìƒ‰ ì‹œì‘: query='{query[:50]}...'")

            # ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            context = self._create_search_context(
                query, max_results, similarity_threshold
            )

            # ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰
            tasks = []
            task_names = []

            if include_chroma:
                tasks.append(self._search_chroma_async(context))
                task_names.append("chroma")
            if include_pdf_realtime:
                tasks.append(self._search_pdf_realtime_async(context))
                task_names.append("pdf_realtime")

            if not tasks:
                log.warning("ì‹¤í–‰í•  ê²€ìƒ‰ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")
                return []

            # asyncio.gatherë¡œ ë³‘ë ¬ ì‹¤í–‰
            parallel_start = time.time()
            search_results_with_exceptions = await asyncio.gather(
                *tasks, return_exceptions=True
            )
            parallel_time = time.time() - parallel_start
            log.info(
                f"âš¡ ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰ ì™„ë£Œ: {len(tasks)}ê°œ ì‘ì—…, ì†Œìš”ì‹œê°„={parallel_time:.3f}s"
            )

            # ì˜ˆì™¸ ì²˜ë¦¬ ë° ê²°ê³¼ ìˆ˜ì§‘
            search_results = []
            for i, result in enumerate(search_results_with_exceptions):
                if isinstance(result, Exception):
                    log.error(f"{task_names[i]} ê²€ìƒ‰ ì‹¤íŒ¨: {result}")
                    search_results.append([])  # ë¹ˆ ê²°ê³¼ë¡œ ì²˜ë¦¬
                else:
                    search_results.append(result)

            # RAG ê²°ê³¼ í†µí•© ë° í›„ì²˜ë¦¬
            unified_results = self._integrate_results(search_results)

            # ì»¨í…ìŠ¤íŠ¸ ìµœì í™” ë° ì¤‘ë³µ ì œê±°
            final_results = self._optimize_context(
                unified_results, context.max_context_length
            )

            search_time = time.time() - search_start
            log.info(
                f"âœ… ë¹„ë™ê¸° RAG ê²€ìƒ‰ ì™„ë£Œ: {len(final_results)}ê°œ ê²°ê³¼, "
                f"ì†Œìš”ì‹œê°„={search_time:.3f}s (ë³‘ë ¬ ì²˜ë¦¬)"
            )

            return final_results

        except Exception as e:
            log.error(f"ë¹„ë™ê¸° í†µí•© ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            raise

    def _create_search_context(
        self,
        query: str,
        max_results: Optional[int],
        similarity_threshold: Optional[float],
    ) -> SearchContext:
        """ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            context_start = time.time()
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self._get_query_embedding(query)

            # ì„¤ì •ê°’ ì ìš©
            max_results = max_results or self.chroma_client.max_results
            similarity_threshold = similarity_threshold or self.similarity_threshold

            context = SearchContext(
                query=query,
                query_embedding=query_embedding,
                max_results=max_results,
                similarity_threshold=similarity_threshold,
                max_context_length=self.max_context_length,
            )

            context_time = time.time() - context_start
            log.info(f"ğŸ“‹ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ: {context_time:.3f}s")
            return context
        except Exception as e:
            log.error(f"ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            raise

    def _get_query_embedding(self, query: str) -> List[float]:
        """ì¿¼ë¦¬ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            embedding_start = time.time()
            # ì§ì ‘ ì„ë² ë”© ìƒì„±
            embedding = self.embedding_model.encode_text(query)

            embedding_time = time.time() - embedding_start
            log.info(f"ğŸ”¢ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì™„ë£Œ: {embedding_time:.3f}s")

            return embedding.tolist()
        except Exception as e:
            log.error(f"ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            raise

    async def _search_chroma_async(
        self, context: SearchContext
    ) -> List[UnifiedSearchResult]:
        """ë¹„ë™ê¸° í¬ë¡œë§ˆ DB ê²€ìƒ‰ì„ ì‹¤í–‰í•©ë‹ˆë‹¤ (ê³ ê¸‰ ë­í‚¹ ë° ì¤‘ë³µ ì œê±° í¬í•¨)."""
        try:
            chroma_start = time.time()
            # I/O ë°”ìš´ë“œ ì‘ì—…ì„ ìŠ¤ë ˆë“œ í’€ì—ì„œ ì‹¤í–‰
            loop = asyncio.get_event_loop()

            # ChromaDB ì¿¼ë¦¬ë¥¼ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
            search_result = await loop.run_in_executor(
                None,
                lambda: self.chroma_client.query_with_ranking(
                    query_text=context.query,
                    top_k=context.max_results,
                    where=None,
                    include_documents=True,
                    remove_duplicates=True,  # ChromaDB ìˆ˜ì¤€ì—ì„œ ì˜ë¯¸ì  ì¤‘ë³µ ì œê±°
                    similarity_threshold=0.95,  # ì¤‘ë³µ íŒë‹¨ ì„ê³„ê°’ (95% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì¤‘ë³µ)
                ),
            )

            # UnifiedSearchResultë¡œ ë³€í™˜
            unified_results = []
            for i, (doc_id, document, metadata, distance) in enumerate(
                zip(
                    search_result.ids,
                    search_result.documents,
                    search_result.metadatas,
                    search_result.distances,
                )
            ):
                # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ ì ìˆ˜ë¡œ ë³€í™˜ (cosine distance -> similarity)
                similarity_score = 1.0 - distance

                # ê²€ìƒ‰ ì„ê³„ê°’ í•„í„°ë§ (ì—¬ì „íˆ ì ìš©)
                if similarity_score >= context.similarity_threshold:
                    unified_results.append(
                        UnifiedSearchResult(
                            content=document,
                            source_type="chroma",
                            source_id=doc_id,
                            metadata=metadata or {},
                            similarity_score=similarity_score,
                            rank=i + 1,  # ì´ë¯¸ ì •ë ¬ëœ ìˆœì„œ
                            context_length=len(document),
                        )
                    )

            chroma_time = time.time() - chroma_start
            log.info(
                f"ğŸ—„ï¸ ë¹„ë™ê¸° í¬ë¡œë§ˆ DB ê²€ìƒ‰ ì™„ë£Œ: {len(unified_results)}ê°œ ê²°ê³¼, "
                f"ì†Œìš”ì‹œê°„={chroma_time:.3f}s (ì •ë ¬ ë° ì¤‘ë³µ ì œê±° ì ìš©ë¨)"
            )
            return unified_results

        except Exception as e:
            log.error(f"ë¹„ë™ê¸° í¬ë¡œë§ˆ DB ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    async def _search_pdf_realtime_async(
        self, context: SearchContext
    ) -> List[UnifiedSearchResult]:
        """ë¹„ë™ê¸° ì‹¤ì‹œê°„ PDF ê²€ìƒ‰ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        try:
            pdf_start = time.time()
            # I/O ë°”ìš´ë“œ ì‘ì—…ì„ ìŠ¤ë ˆë“œ í’€ì—ì„œ ì‹¤í–‰
            loop = asyncio.get_event_loop()

            # PDF ê²€ìƒ‰ì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
            pdf_chunks = await loop.run_in_executor(
                None,
                lambda: self.pdf_processor.search_chunks(
                    query=context.query, top_k=context.max_results, file_filter=None
                ),
            )

            # ìœ ì‚¬ë„ ì„ê³„ê°’ í•„í„°ë§
            filtered_chunks = [
                (chunk, score)
                for chunk, score in pdf_chunks
                if score >= context.similarity_threshold
            ]

            # UnifiedSearchResultë¡œ ë³€í™˜
            unified_results = []
            for i, (chunk, similarity_score) in enumerate(filtered_chunks):
                unified_results.append(
                    UnifiedSearchResult(
                        content=chunk.text,
                        source_type="pdf_realtime",
                        source_id=chunk.id,
                        metadata={
                            **chunk.metadata,
                            "page": chunk.page,
                            "chunk_index": chunk.chunk_index,
                            "created_at": chunk.created_at,
                        },
                        similarity_score=similarity_score,
                        rank=i + 1,
                        context_length=len(chunk.text),
                    )
                )

            pdf_time = time.time() - pdf_start
            log.info(
                f"ğŸ“„ ë¹„ë™ê¸° ì‹¤ì‹œê°„ PDF ê²€ìƒ‰ ì™„ë£Œ: {len(unified_results)}ê°œ ê²°ê³¼, ì†Œìš”ì‹œê°„={pdf_time:.3f}s"
            )
            return unified_results

        except Exception as e:
            log.error(f"ë¹„ë™ê¸° ì‹¤ì‹œê°„ PDF ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def _integrate_results(
        self, search_results: List[List[UnifiedSearchResult]]
    ) -> List[UnifiedSearchResult]:
        """RAG ë‹¤ì¤‘ ì†ŒìŠ¤ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í†µí•©í•©ë‹ˆë‹¤."""
        try:
            integrate_start = time.time()
            all_results = []

            # ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì§‘
            for result in search_results:
                all_results.extend(result)

            if not all_results:
                log.warning("í†µí•©í•  RAG ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []

            # ì ìˆ˜ ì •ê·œí™” ë° ë­í‚¹ ì¬ê³„ì‚°
            normalized_results = self._normalize_scores(all_results)

            # ì†ŒìŠ¤ë³„ ê°€ì¤‘ì¹˜ ì ìš©
            weighted_results = self._apply_source_weights(normalized_results)

            # ìµœì¢… ë­í‚¹
            final_results = sorted(
                weighted_results, key=lambda x: x.similarity_score, reverse=True
            )

            # ì¤‘ë³µ ì œê±° ì ìš©
            final_results = self._remove_duplicates(final_results)

            integrate_time = time.time() - integrate_start
            log.info(
                f"ğŸ”— RAG ê²°ê³¼ í†µí•© ì™„ë£Œ: {len(final_results)}ê°œ ê²°ê³¼, ì†Œìš”ì‹œê°„={integrate_time:.3f}s"
            )
            return final_results

        except Exception as e:
            log.error(f"ê²°ê³¼ í†µí•© ì‹¤íŒ¨: {e}")
            return []

    def _normalize_scores(
        self, results: List[UnifiedSearchResult]
    ) -> List[UnifiedSearchResult]:
        """ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ì •ê·œí™”í•©ë‹ˆë‹¤."""
        try:
            if not results:
                return results

            # ì ìˆ˜ ë²”ìœ„ ì •ê·œí™” (0-1)
            scores = [r.similarity_score for r in results]
            min_score, max_score = min(scores), max(scores)

            if max_score == min_score:
                # ëª¨ë“  ì ìˆ˜ê°€ ë™ì¼í•œ ê²½ìš° ê· ë“± ë¶„ë°°
                normalized_score = 0.5
                for result in results:
                    result.similarity_score = normalized_score
            else:
                # Min-Max ì •ê·œí™”
                for result in results:
                    result.similarity_score = (result.similarity_score - min_score) / (
                        max_score - min_score
                    )

            return results

        except Exception as e:
            log.error(f"ì ìˆ˜ ì •ê·œí™” ì‹¤íŒ¨: {e}")
            return results

    def _apply_source_weights(
        self, results: List[UnifiedSearchResult]
    ) -> List[UnifiedSearchResult]:
        """RAG ì†ŒìŠ¤ë³„ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•©ë‹ˆë‹¤."""
        try:
            # RAG ì†ŒìŠ¤ë³„ ê°€ì¤‘ì¹˜ ì •ì˜ (MCP, Google ì œì™¸)
            source_weights = {
                "chroma": 1.0,  # ê¸°ì¡´ ì €ì¥ ë¬¸ì„œ (ê¸°ë³¸ ê°€ì¤‘ì¹˜)
                "pdf_realtime": 1.2,  # ìƒˆë¡œ ì—…ë¡œë“œëœ ë¬¸ì„œ (ë†’ì€ ê°€ì¤‘ì¹˜)
            }

            for result in results:
                weight = source_weights.get(result.source_type, 1.0)
                result.similarity_score *= weight

            return results

        except Exception as e:
            log.error(f"RAG ì†ŒìŠ¤ ê°€ì¤‘ì¹˜ ì ìš© ì‹¤íŒ¨: {e}")
            return results

    def _optimize_context(
        self, results: List[UnifiedSearchResult], max_context_length: int
    ) -> List[UnifiedSearchResult]:
        """ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤."""
        optimize_start = time.time()
        try:
            if not results:
                return []

            # í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ê³„ì‚°
            current_length = sum(len(result.content) for result in results)

            if current_length <= max_context_length:
                return results

            # ê¸¸ì´ ì œí•œì„ ì´ˆê³¼í•˜ëŠ” ê²½ìš°, ì¤‘ìš”ë„ ê¸°ë°˜ìœ¼ë¡œ ì„ íƒ
            optimized = []
            accumulated_length = 0

            for result in results:
                content_length = len(result.content)

                # ì´ ê²°ê³¼ë¥¼ ì¶”ê°€í•´ë„ ì œí•œì„ ì´ˆê³¼í•˜ì§€ ì•ŠëŠ” ê²½ìš°
                if accumulated_length + content_length <= max_context_length:
                    optimized.append(result)
                    accumulated_length += content_length
                else:
                    # ë‚¨ì€ ê³µê°„ì— ë§ëŠ”ì§€ í™•ì¸
                    remaining_space = max_context_length - accumulated_length
                    if remaining_space > 100:  # ìµœì†Œ 100ì ì´ìƒ ë‚¨ì€ ê²½ìš°ë§Œ
                        # ê²°ê³¼ë¥¼ ì˜ë¼ì„œ ì¶”ê°€
                        truncated_result = UnifiedSearchResult(
                            content=result.content[:remaining_space] + "...",
                            source_type=result.source_type,
                            source_id=result.source_id,
                            metadata=result.metadata,
                            similarity_score=result.similarity_score,
                            rank=result.rank,
                            context_length=remaining_space,
                        )
                        optimized.append(truncated_result)
                    break

            # ì¤‘ë³µ ì œê±° ì ìš©
            optimized = self._remove_duplicates(optimized)
            optimize_time = time.time() - optimize_start
            log.info(
                f"âš¡ ì»¨í…ìŠ¤íŠ¸ ìµœì í™” ì™„ë£Œ: {len(optimized)}ê°œ ê²°ê³¼, ì†Œìš”ì‹œê°„={optimize_time:.3f}s"
            )

            return optimized

        except Exception as e:
            log.error(f"âŒ ì»¨í…ìŠ¤íŠ¸ ìµœì í™” ì‹¤íŒ¨: {e}")
            return results[:5]  # ê¸°ë³¸ì ìœ¼ë¡œ ìƒìœ„ 5ê°œë§Œ ë°˜í™˜

    def _remove_duplicates(
        self, results: List[UnifiedSearchResult]
    ) -> List[UnifiedSearchResult]:
        """ì¤‘ë³µ ë‚´ìš©ì„ ì œê±°í•©ë‹ˆë‹¤ (ì†ŒìŠ¤ë³„ ìµœì í™” ì ìš©)."""
        try:
            dedup_start = time.time()
            # ì†ŒìŠ¤ë³„ë¡œ ë¶„ë¥˜
            chroma_results = [r for r in results if r.source_type == "chroma"]
            other_results = [r for r in results if r.source_type != "chroma"]

            # ChromaDB ê²°ê³¼ëŠ” ì´ë¯¸ ì˜ë¯¸ì  ì¤‘ë³µ ì œê±°ê°€ ì ìš©ë˜ì—ˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ìœ ì§€
            deduplicated = chroma_results.copy()

            # ë‹¤ë¥¸ ì†ŒìŠ¤ ê²°ê³¼ë“¤ì— ëŒ€í•´ì„œë§Œ ë¬¸ìì—´ ê¸°ë°˜ ì¤‘ë³µ ì œê±° ì ìš©
            seen_contents = set()

            # ChromaDB ê²°ê³¼ì˜ ë‚´ìš©ì„ seen_contentsì— ì¶”ê°€
            for result in chroma_results:
                normalized_content = self._normalize_content(result.content)
                seen_contents.add(normalized_content)

            # ë‹¤ë¥¸ ì†ŒìŠ¤ ê²°ê³¼ ì¤‘ë³µ ì œê±°
            for result in other_results:
                normalized_content = self._normalize_content(result.content)

                if normalized_content not in seen_contents:
                    seen_contents.add(normalized_content)
                    deduplicated.append(result)

            dedup_time = time.time() - dedup_start
            log.info(
                f"ğŸ”„ ì†ŒìŠ¤ë³„ ì¤‘ë³µ ì œê±°: {len(results)} â†’ {len(deduplicated)}ê°œ, "
                f"ì†Œìš”ì‹œê°„={dedup_time:.3f}s "
                f"(ChromaDB: {len(chroma_results)}ê°œ ìœ ì§€, ê¸°íƒ€: {len(deduplicated) - len(chroma_results)}ê°œ)"
            )
            return deduplicated

        except Exception as e:
            log.error(f"ì¤‘ë³µ ì œê±° ì‹¤íŒ¨: {e}")
            return results

    def _normalize_content(self, content: str) -> str:
        """ë‚´ìš©ì„ ì •ê·œí™”í•˜ì—¬ ë¹„êµìš©ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤."""
        try:
            # ê¸°ë³¸ ì •ê·œí™”
            normalized = content.lower().strip()

            # ê³µë°± ì •ê·œí™”
            import re

            normalized = re.sub(r"\s+", " ", normalized)

            # íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ìœ ì§€)
            normalized = re.sub(r"[^\w\sê°€-í£]", "", normalized)

            return normalized

        except Exception as e:
            log.error(f"ë‚´ìš© ì •ê·œí™” ì‹¤íŒ¨: {e}")
            return content

    def get_search_summary(self, results: List[UnifiedSearchResult]) -> Dict[str, Any]:
        """ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        try:
            if not results:
                return {"total_results": 0, "sources": {}, "avg_score": 0.0}

            # ì†ŒìŠ¤ë³„ í†µê³„
            source_stats = {}
            total_score = 0.0

            for result in results:
                source_type = result.source_type
                if source_type not in source_stats:
                    source_stats[source_type] = {
                        "count": 0,
                        "total_score": 0.0,
                        "avg_score": 0.0,
                    }

                source_stats[source_type]["count"] += 1
                source_stats[source_type]["total_score"] += result.similarity_score
                total_score += result.similarity_score

            # í‰ê·  ì ìˆ˜ ê³„ì‚°
            for source_type in source_stats:
                count = source_stats[source_type]["count"]
                total = source_stats[source_type]["total_score"]
                source_stats[source_type]["avg_score"] = total / count

            return {
                "total_results": len(results),
                "sources": source_stats,
                "avg_score": total_score / len(results),
                "context_length": sum(r.context_length for r in results),
            }

        except Exception as e:
            log.error(f"ê²€ìƒ‰ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def __del__(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        pass

    async def aclose(self) -> None:
        """ë¹„ë™ê¸° ë¦¬ì†ŒìŠ¤ ì •ë¦¬ í•¨ìˆ˜

        VectorSearchManagerê°€ ì‚¬ìš©í•˜ëŠ” ì™¸ë¶€ ë¦¬ì†ŒìŠ¤(ì˜ˆ: DB ì»¤ë„¥ì…˜, ì„ë² ë”© ëª¨ë¸ ë“±)ë¥¼ ì•ˆì „í•˜ê²Œ í•´ì œí•©ë‹ˆë‹¤.
        """
        try:
            # ì˜ˆì‹œ: ChromaDBClient, PDFProcessor, EmbeddingModelì— ë¹„ë™ê¸° close ë©”ì„œë“œê°€ ìˆë‹¤ê³  ê°€ì •
            if hasattr(self, "chroma_client") and hasattr(self.chroma_client, "aclose"):
                await self.chroma_client.aclose()
            if hasattr(self, "pdf_processor") and hasattr(self.pdf_processor, "aclose"):
                await self.pdf_processor.aclose()
            if (
                hasattr(self, "embedding_model")
                and hasattr(self.embedding_model, "aclose")
            ):
                await self.embedding_model.aclose()
            log.info("VectorSearchManager ë¦¬ì†ŒìŠ¤ ë¹„ë™ê¸° ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            log.error(f"VectorSearchManager ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")



# í¸ì˜ í•¨ìˆ˜ë“¤
def create_vector_search_manager(
    chroma_client: Optional[ChromaDBClient] = None,
    pdf_processor: Optional[PDFProcessor] = None,
    embedding_model: Optional[KoreanEmbeddingModel] = None,
    **kwargs,
) -> VectorSearchManager:
    """VectorSearchManagerë¥¼ ìƒì„±í•˜ëŠ” í¸ì˜ í•¨ìˆ˜ì…ë‹ˆë‹¤."""
    try:
        factory_start = time.time()

        manager = VectorSearchManager(
            chroma_client=chroma_client,
            pdf_processor=pdf_processor,
            embedding_model=embedding_model,
            **kwargs,
        )

        factory_time = time.time() - factory_start
        log.info(f"ğŸ­ VectorSearchManager íŒ©í† ë¦¬ ìƒì„± ì™„ë£Œ: {factory_time:.3f}s")

        return manager
    except Exception as e:
        log.error(f"VectorSearchManager ìƒì„± ì‹¤íŒ¨: {e}")
        raise


async def search_rag_async(
    query: str, vector_manager: Optional[VectorSearchManager] = None, **kwargs
) -> List[UnifiedSearchResult]:
    """ë¹„ë™ê¸° RAG ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” í¸ì˜ í•¨ìˆ˜ì…ë‹ˆë‹¤."""
    try:
        convenience_start = time.time()

        if vector_manager is None:
            manager_start = time.time()
            vector_manager = create_vector_search_manager()
            manager_time = time.time() - manager_start
            log.info(f"ğŸ—ï¸ VectorSearchManager ìƒì„± ì™„ë£Œ: {manager_time:.3f}s")

        result = await vector_manager.search_async(query, **kwargs)

        convenience_time = time.time() - convenience_start
        log.info(
            f"ğŸ¯ RAG í¸ì˜ í•¨ìˆ˜ ì‹¤í–‰ ì™„ë£Œ: {len(result)}ê°œ ê²°ê³¼, ì´ ì†Œìš”ì‹œê°„={convenience_time:.3f}s"
        )

        return result
    except Exception as e:
        log.error(f"ë¹„ë™ê¸° RAG ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        raise
